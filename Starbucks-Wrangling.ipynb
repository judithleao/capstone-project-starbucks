{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2707b931",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "There are standard data exploring and cleaning (section \"Inspecting data\") and basic data wrangling procedures (Step 1 of \"Transforming Data\") contained in this script. However, some heavier data transformations also happen in the \"Transforming Data\" section, with the most complex parts being 3 and 5. In step 3, view and complete events are allocated to offer received events. Block ids for these sets of events that identify which receive, view and complete belong together are created. In step 5, the transactions are allocated to these blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a732640",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d20b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "#% matplotlib inline\n",
    "from pandasql import sqldf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466cc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\j\\Documents\\udacity-ds-nanodegree\\starbucks-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5984fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bd903",
   "metadata": {},
   "source": [
    "## Inspecting data\n",
    "\n",
    "In this section, I inspect all three dataframes to assert data quality, define steps for data cleaning, and get a first impression of the data.\n",
    "Further exploration can only be carried out after complex data transformations, which will also be required to carry out the actual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84e8e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd93260",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "portfolio.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610f505",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transcript.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95246c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### profile data\n",
    "\n",
    "#### Summary of inspection of data in profile table:\n",
    "* As I cannot judge if this is a representative sample of the Starbucks App user population, I will assume it is.\n",
    "* The table only contains **unique records** of app-users. No deduplication is needed.\n",
    "* Around 12.8% of these users have **missing information** for all three demographic variables. I believe that this data should not be imputed. It cannot be *derived/predicted* from any other variables, as all other variables will be part of the prediction model. In addition, these demographic values will be important predictors for the model themselves, therefore imputation like using the mean/median or mode seems too \"rough\" an approach. I will instead check if the people without information are systematically different from those with information, and if not, I will drop the records. In order to make this assessment, the data needs to be transformed.\n",
    "* The **gender** is not 50-50, as would be expected from the general population. It is, however, possible that the split is representative of the wider Starbucks App subscriber population.\n",
    "* The **age** distribution suggests that the sample is rather old, which I find surprising for Stabrucks users. Other than that the age range seems plausible.\n",
    "* The  **income** appears to have been capped both at the lower and higher end. Other than that the distribution seems plausible, showing the typical right skew that income data often displays.\n",
    "* The **membership** column has no missings and the data range is plausible. Most memberships are more recent, which makes sense considering that consumers probably drop out after a certain amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17670d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Number of unique users, check for duplicates\n",
    "print(\"Number of unique user: \",profile['id'].nunique())\n",
    "print(\"Number of users in df: \",profile.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba94a9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assess \"gender\" column\n",
    "print(\"Categorical codes for gender column: \",profile['gender'].value_counts())\n",
    "print(\"Number of missing values in gender column: \",profile['gender'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb89a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assess \"age\" column distribution, code 118 is missing information\n",
    "profile['age'].hist() # >> surprisingly old population considering company is Starbucks\n",
    "\n",
    "# Recode 118 inta np.nan\n",
    "profile.loc[profile['age'] == 118, 'age'] = np.nan\n",
    "\n",
    "# get min and max values, and missing values\n",
    "print(\"Lowest age in dataset: \",profile['age'].min())\n",
    "print(\"Highest age in dataset: \", profile['age'].max())\n",
    "print(\"Number of missing values in age column: \",profile['age'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918bb9b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assess \"income\" column distribution\n",
    "profile['income'].hist() # >> I assume income was cropped at a min and max value\n",
    "\n",
    "# get min and max values, and missing values\n",
    "print(\"Lowest age in dataset: \",profile['income'].min())\n",
    "print(\"Highest age in dataset: \", profile['income'].max())\n",
    "print(\"Number of missing values in income column: \",profile['income'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86edc83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check if the missings are the same for all three columns\n",
    "all_missing = profile.loc[(profile['income'].isna()) & (profile['age'].isna()) & (profile['gender'].isna())].shape[0]\n",
    "print(\"Records that have all three demographic attributes missing: \",all_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953c7f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assess share of records without demographic information out of all records\n",
    "all_missing/profile.shape[0] #>> I would lose 12.8% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7851f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Need to assess if those who did not provide demographic data are systematically different from those who did to decide if I can remove them from the sample. I will compare:\n",
    "* Total spend and offers completed: Those are variables that I will **predict**, so there should not be any bias in them\n",
    "* offers received and type of offer received: That is the **experimental setup** and I do not want this to be biased\n",
    "In order to assess this, I need to maipulate the transcipt df a bit. I need to extract amount for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da1092",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Assess \"membership\" column\n",
    "# Change to datetime format\n",
    "'''\n",
    "Source: https://datatofish.com/integers-datetime-pandas-dataframe/, Auagust 14th 2021\n",
    "'How to convert Integers to datetime in Pandas DataFrame'\n",
    "'''\n",
    "profile['became_member_on'] = pd.to_datetime(profile['became_member_on'], format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffa5c0",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Earliest membership date: \",profile['became_member_on'].min())\n",
    "print(\"Latest membership date: \",profile['became_member_on'].max())\n",
    "profile['became_member_on'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24450be7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### portfolio data\n",
    "\n",
    "#### Summary of inspection of data in portfolio table\n",
    "* There is total of 10 offers\n",
    "* Intuitively, the expectation would be that a reward rises with difficulty, and that duration rises with difficulty. \n",
    "* When examining the data through scatterplots, this relationship is not actually that clear. There are some offers that appear  less attractive than others.\n",
    "* However, these offers vary on further attributes, like type and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4eafba",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "portfolio.head(10).sort_values(by=['offer_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2dd9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check uniqueness of offer ids\n",
    "portfolio['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7c492",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Number of offers per type\n",
    "portfolio['offer_type'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cefd2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# examining relationship between attributes\n",
    "plt.scatter(portfolio['reward'], portfolio['difficulty'])\n",
    "plt.xlabel('reward')\n",
    "plt.ylabel('difficulty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159c96a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# examining relationship between attributes\n",
    "plt.scatter(portfolio['duration'], portfolio['difficulty'])\n",
    "plt.xlabel('duration')\n",
    "plt.ylabel('difficulty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b005fb",
   "metadata": {},
   "source": [
    "### transaction data\n",
    "\n",
    "#### Summary of inspection of data in transaction table\n",
    "* The logical funnel for an offer is *receive >> view >> complete*\n",
    "* However, this funnel can be shortened to *receive >> view* and also to *receive >> complete*, or switched to *receive >> complete >> view*\n",
    "* The maximum number of offers someone can receive is 6\n",
    "* Most people receive 5 offers\n",
    "* Most people view 3 offers\n",
    "* Most people complete 2 offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adcdb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many offers does a person receive >> histogram\n",
    "offers_h = transcript.loc[transcript['event'] == 'offer received'].groupby(['person']).size()\n",
    "views_h = transcript.loc[transcript['event'] == 'offer viewed'].groupby(['person']).size()\n",
    "completes_h = transcript.loc[transcript['event'] == 'offer completed'].groupby(['person']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26675d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of offers, needed to define bins in histogram\n",
    "offers_h.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02daae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version with transparent bars - user version from below, just keep this code for reference. Alpha makes transparent, the lower alpha the more transparent.\n",
    "plt.hist(offers_h, bins = [1,2,3,4,5,6,7],alpha = 0.3, align = 'left')\n",
    "plt.hist(views_h, bins = [1,2,3,4,5,6,7], alpha = 0.3, align = 'left')\n",
    "plt.hist(completes_h, bins = [1,2,3,4,5,6,7], alpha = 0.3, align = 'left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dac770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version with bars side by side\n",
    "# Source Stackoverflow: https://stackoverflow.com/questions/6871201/plot-two-histograms-on-single-chart-with-matplotlib\n",
    "# Question by Open the way: https://stackoverflow.com/users/171546/open-the-way\n",
    "# Answer by Gustavo Bezerra: https://stackoverflow.com/users/2132753/gustavo-bezerra\n",
    "plt.hist([offers_h, views_h, completes_h], bins = [1,2,3,4,5,6,7], align = 'left', label = ['received', 'viewed', 'completed'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65081184",
   "metadata": {},
   "source": [
    "## Transforming data\n",
    "* **Step 1:** Wrangle all three dataframes and create master df which is based on transaction frame but without transactions. This df will be transformed to be fit for analysis.\n",
    "* **Step 2:** Answer some relevant questions about the general experimental setup\n",
    "* **Step 3:** Transform master df to create a table which has one record per person and offer, and which contains additional information on this particular offer.\n",
    "* **Step 4:** Create derived variables\n",
    "* **Step 5:** Add transaction value dataframe: matrix person on offer and no_offer and avg value.\n",
    "Start with a person x hour df where each hour has a 0 (no offer) to 10, and then you can use that to lookup the transaction's value maybe? Too simple I guess, check your transaction allocation logic again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5f9a2",
   "metadata": {},
   "source": [
    "### Step 1: Wrangling all three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309de7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# profile data\n",
    "'''\n",
    "Source: https://datatofish.com/integers-datetime-pandas-dataframe/, Auagust 14th 2021\n",
    "'How to convert Integers to datetime in Pandas DataFrame'\n",
    "\n",
    "Source: Stackoverflow: https://stackoverflow.com/questions/53986451/calculate-date-difference-between-todays-date-and-pandas-date-series/53986547\n",
    "Question by Shivam: https://stackoverflow.com/users/10851563/shivam\n",
    "Answer by k88: https://stackoverflow.com/users/5682512/k88\n",
    "\n",
    "Source: Satckoverflow: https://stackoverflow.com/questions/2119472/convert-a-timedelta-to-days-hours-and-minutes\n",
    "Question by Oli: https://stackoverflow.com/users/12870/oli\n",
    "Answer by Alex Martelli: https://stackoverflow.com/users/95810/alex-martelli\n",
    "'''\n",
    "# Calculating duration of membership\n",
    "profile['became_member_on'] = pd.to_datetime(profile['became_member_on'], format = '%Y%m%d')\n",
    "profile['membership_duration'] = profile['became_member_on'] - pd.to_datetime(\"now\")\n",
    "profile['membership_duration'] = profile['membership_duration'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09801371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# portfolio data\n",
    "# Calculating until when offer is valid\n",
    "portfolio = portfolio.rename(columns={'reward':'offer_reward','channels':'offer_channels','difficulty':'offer_difficulty', 'duration':'offer_duration'})\n",
    "portfolio['offer_duration'] = portfolio['offer_duration']*24\n",
    "portfolio['offer_valid_until'] = np.nan\n",
    "\n",
    "\n",
    "# One-hot-encoding channels\n",
    "# Source Stackoverflow: https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "# Question by Melsauce: https://stackoverflow.com/users/5858873/melsauce\n",
    "# Answer by piRsquared: https://stackoverflow.com/users/2336654/pirsquared\n",
    "portfolio = portfolio.drop('offer_channels',1).join(portfolio['offer_channels'].str.join('|').str.get_dummies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016d943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data >> create master frame\n",
    "'''\n",
    "Source: Stackoverflow: https://stackoverflow.com/questions/38231591/split-explode-a-column-of-dictionaries-into-separate-columns-with-pandas\n",
    "Question by Ilaffin: https://stackoverflow.com/users/6157698/llaffin\n",
    "Answer by joris: https://stackoverflow.com/users/653364/joris\n",
    "'''\n",
    "\n",
    "# Turning the value column which lists different events into separate columns\n",
    "master = pd.concat([transcript, pd.DataFrame(transcript['value'].tolist())], axis=1)\n",
    "\n",
    "# There are cases where the offer id is offer_id in the 'value' dictionary. Need to combine both columns.\n",
    "'''\n",
    "Source: Stackoverflow: https://stackoverflow.com/questions/34989341/how-to-remove-nan-value-while-combining-two-column-in-panda-data-frame\n",
    "Question by imSonuGupta: https://stackoverflow.com/users/3101669/imsonugupta\n",
    "Answer by jpp: https://stackoverflow.com/users/9209546/jpp\n",
    "'''\n",
    "master['offer id'].update(master.pop('offer_id'))\n",
    "master = master.rename(columns={'offer id':'event_offer_id','amount':'event_amount','reward':'event_reward'})\n",
    "\n",
    "# Removing all transaction records\n",
    "master = master.loc[master['event'] != 'transaction']\n",
    "# Remove event_amount column as not needed\n",
    "master = master.drop('event_amount', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging portfolio table\n",
    "master = pd.merge(master, portfolio[['offer_type','offer_reward','offer_difficulty','offer_duration','email','web','social','mobile']], how = 'left', left_on = master['event_offer_id'], right_on = portfolio['id'])\n",
    "master = master.drop('key_0', axis = 1)\n",
    "\n",
    "# Creating derived variable offer_valid_until\n",
    "master.loc[master['event'] == 'offer received', 'offer_valid_until'] = master['time'] + master['offer_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb89cc7",
   "metadata": {},
   "source": [
    "### Step 2: Understand experiment setup in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04132f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### When do offers get sent out? >> At 6 distinct points in time. They are not spaced out evenly, and a slightly different amount of offers is sent out each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f56ce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "master_receive = master.loc[master['event'] == 'offer received']\n",
    "master_receive_gr = pd.DataFrame(master_receive.groupby(['time']).size().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef60e85",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "master_receive_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defb5e7",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minor_ticks = np.arange(0,600,24)\n",
    "plt.scatter(master_receive_gr['time'],master_receive_gr[0])\n",
    "plt.grid(axis = 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b9dd3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Does a specific offer only get sent out at one point in time? >> No\n",
    "#### Do different offers get sent out at the same time? >> Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8300a5",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(master_receive['event_offer_id'],master_receive['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1b88e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Can a person receive the same offer multiple times? >> Yes, the maximum a person has received the same offer is 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d59c3b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(master_receive['person'], master_receive['event_offer_id']).max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87aa6b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Can a person receive a new offer whilst another one is still active? >> Yes, this happens frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d1792",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Creating subset\n",
    "offer_logic_check = master_receive[['person','event_offer_id','time','offer_valid_until']].sort_values(by=['person', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22383631",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " # shifting row values one row down\n",
    "offer_logic_check['shift_time'] = offer_logic_check['offer_valid_until'].shift(periods = 1)\n",
    "offer_logic_check['shift_person'] = offer_logic_check['person'].shift(periods = 1)\n",
    "# Create checking column, populate only with 0\n",
    "offer_logic_check['previous_offer_still_valid'] = 'no overlap'\n",
    "# Populate checking column with 1 if new offer overlaps with previous offer\n",
    "offer_logic_check.loc[(offer_logic_check['shift_person'] == offer_logic_check['person']) & (offer_logic_check['shift_time'] > offer_logic_check['time']),'previous_offer_still_valid'] = 'overlap'\n",
    "# Count occurences where new offer overlaps with previous offer\n",
    "offer_logic_check['previous_offer_still_valid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2838ad0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Can a person receive a new offer whilst still being aware of an active, not completed offer? >> Yes, seen upon visual inspection of the data\n",
    "#### If yes, can this be the *same* offer? >> Yes, seen upon visual inspection of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a9274",
   "metadata": {},
   "source": [
    "### Step 3: Wrangle master table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cbec43",
   "metadata": {},
   "source": [
    "#### Step 3.1 Identify unique blocks of offer receive events with views and completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b237b5c",
   "metadata": {},
   "source": [
    "I need to be able to uniquely identify an \"offer block\". An offer block defines a person-x-offer combination, where an offer block starts with a receive and contains *exactly* one receive, and a maximum of one view and one complete. I will achieve this in two stages:\n",
    "* In a first step, I will assign an ID to each offer block that uniquely identifies a person x offer combination\n",
    "* Following this, I will split up offer blocks that still contain multiple receives because a person received a specific offer multiple times.\n",
    "\n",
    "Finally, I will do some data checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e3dd2",
   "metadata": {},
   "source": [
    "##### Step 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the master dataframe. This is essential for the further steps.\n",
    "master = master.sort_values(by=['person', 'event_offer_id', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39636905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that creates a unique identifier for each person and offer combination\n",
    "person_offer_id_df = master.loc[master['event'] == 'offer received', ['person', 'event_offer_id']].drop_duplicates().reset_index().rename(columns={'index':'person_offer_id'})\n",
    "# Merge the id back into the master dataframe\n",
    "master = pd.merge(master, person_offer_id_df, how = 'left', left_on = ['person','event_offer_id'], right_on = ['person','event_offer_id'])\n",
    "# Note: This only affect event: receive, view and complete, because transactions do not have an event_offer_id. This will be tackled later-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because one person can receive the same offer multiple times, some offer blocks will have multiple offer received. \n",
    "# The most times one person has received the same offer is:\n",
    "max_same_offer = master.loc[master['event'] == 'offer received'].groupby(by=['person_offer_id']).size().max()\n",
    "print(\"The most times one person has received the same offer is:\", max_same_offer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d12532",
   "metadata": {},
   "source": [
    "##### Step 3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c088e",
   "metadata": {},
   "source": [
    "Because one person can receive the same offer multiple times, these offer blocks need to be broken down further into offer blocks, where each \"receive\" starts its own offer block. First, I need to identify the person_offer_ids where the block has multiple receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54014e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "receives_per_block = master.loc[master['event'] == 'offer received'].groupby(by=['person_offer_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8928d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "receives_per_block_df = pd.DataFrame(receives_per_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e2c1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blocks_need_subsplit = receives_per_block_df.loc[receives_per_block_df[0] > 1].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135ec51",
   "metadata": {},
   "source": [
    "Now I will loop through these blocks and split them up into separate blocks.\n",
    "Approach:\n",
    "* Create a subset of the master dataframe that only contains records that belong to the specific person_offer_id\n",
    "* Identify indices of \"offer receive\" records.\n",
    "* Loop through each \"offer receive\" record. Allocate views and completes to the receive record. Mark receive, view and complete records that belong together with the same number in the 'sub_block' column.\n",
    "* The allocation logic is:\n",
    "> * Begin the loop at the earliest \"receive\".\n",
    "> * In each loop, allocate a maximum of one view and one complete to the receive.\n",
    "> * The views and completes get allocated to the earliest receive in whose validity time-frame they fall, unless that receive already has a view and complete allocated.\n",
    "> * If a view or complete is \"taken\" by a receive, it cannot be re-allocated to another receive.\n",
    "> * A view and a complete can only ever be allocated to one receive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec376878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish list to store block allocations\n",
    "sub_block_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe761069",
   "metadata": {},
   "outputs": [],
   "source": [
    "for person_offer_id in blocks_need_subsplit: \n",
    "    # create the dataframe-subset of interest\n",
    "    df = master.loc[master['person_offer_id'] == person_offer_id][['event','offer_valid_until','time']]\n",
    "    df['sub_block'] = np.nan\n",
    "    # Create a dictionary to store the indices, valid_untils and timestamps (i.e. time of reception) of receive records\n",
    "    receive_dict = df.loc[df['event'] == 'offer received'][['offer_valid_until','time']].reset_index().to_dict(orient = 'records')\n",
    "  \n",
    "    # Loop through the \"receive\" records and allocate views and completes\n",
    "    for i in range(len(receive_dict)):\n",
    "        # set key variables for filtering\n",
    "        offer_start_time = receive_dict[i]['time']\n",
    "        validity = receive_dict[i]['offer_valid_until']\n",
    "        # filter the block for views and receives within the right timeframe (from offer receive to offer validity end). Only keep such views and completes that do not yet belong to another receive\n",
    "        df_for_loop = df.loc[(df['time'] <= validity) & (df['time'] >= offer_start_time) & (df['event'] != 'offer received') & (df['sub_block'].isna())]\n",
    "        # reset the index so you can access it later\n",
    "        df_for_loop = df_for_loop.reset_index()\n",
    "        # number the rows using sql and select only the first entries\n",
    "        df_from_loop = sqldf(\"SELECT * FROM (SELECT *, row_number() over (partition by event order by time) as counter_check_inner2 FROM df_for_loop) as subtable WHERE counter_check_inner2 = 1\")\n",
    "        \n",
    "        #??????# Add a step where you only keep the \"view\" as belonging to the receive when it precedes the complete? It is currently unclear if s.o. can view after completion\n",
    "        \n",
    "        # extract the indices of the rows that belong to the \"receive\" you are examining in this loop\n",
    "        rows_affected_list = df_from_loop['index'].to_list()\n",
    "\n",
    "        # mark the \"receive\" you are assessing in the original data, so it is clear it belongs to a specific offer block\n",
    "        df.loc[df.index == receive_dict[i]['index'], 'sub_block'] = i \n",
    "        # now mark the \"view\" and \"complete\" rows in the original df, so it is clear they belong to this specific \"receive\"\n",
    "        df.loc[df.index.isin(rows_affected_list), 'sub_block'] = i \n",
    "        #print(df)\n",
    "           \n",
    "    # Create a dictionary with index and block allocation\n",
    "    df = df.reset_index()\n",
    "    allocation = df[['index','sub_block']].to_dict(orient = 'records')\n",
    "    # Append this dictionary to the sub_block_list\n",
    "    sub_block_list.append(allocation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the resulting list into a list of dicts\n",
    "# https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
    "# Question by Emma: https://stackoverflow.com/users/110527/emma\n",
    "# Answer by Alex Martelli: https://stackoverflow.com/users/95810/alex-martelli\n",
    "flat_list = [i for i in sub_block_list for i in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NaNs in flat list with 9999, so I can delete them lateron. \n",
    "# These NaN values are \"views\" of expired offers and are hence irrelevant for offer awareness. They can be deleted.\n",
    "df_flat_list = pd.DataFrame(flat_list)\n",
    "df_flat_list.loc[df_flat_list['sub_block'].isna(), 'sub_block'] = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf093a9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ----------------------   Start of investigation   ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826f4e3",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting my flat list, because it has NaN values\n",
    "df_flat_list = pd.DataFrame(flat_list)\n",
    "df_flat_list.loc[df_flat_list['sub_block'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dc325",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Understanding why there are nans in my flat list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4baac",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# finding person_offer_id of an example case\n",
    "master.loc[41162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4a676",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2x.loc[m2['person_offer_id'] == 191][['person','event','time','offer_duration','event_offer_id','offer_valid_until','sub_block']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78267cae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# checking I created my block correctly >> I did\n",
    "m2.loc[(m2['person'] == '1c587de019994f93a242c6864fd7bc55') & (m2['event_offer_id'] == 'ae264e3637204a6fb9bb56bc8210ddfd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0b8d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Are all of these NaN cases offer views? >> Yes\n",
    "df_flat_list_nans = df_flat_list.loc[df_flat_list['sub_block'].isna()]\n",
    "df_flat_list_nans_checkframe = pd.merge(df_flat_list_nans, m2, how = 'left', left_on = 'index', right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593dcfbd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_flat_list_nans_checkframe['event'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd96d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Result: There can be offer views when the offer is already expired. That makes me believe that there can also be offer views when the offer has been completed - but that is just a guess.\n",
    "The good thing about those offer views is, that I can just ignore them. They do not influence awareness. Therefore, I mark them as 9999 in the flat list before merging.\n",
    "\n",
    "Further thoughts: At the moment, I allocate a view to the first receive, even if that receive is already completed. That could mean that a second offer does not receive the view, but that for the second offer the view would have created awareness. This means that I could \"miss\" periods of awareness. That would mean that i attribute too much spend to the unaware state. However, it is unclear and not clarified anywhere if someone can view an offer after having completed it. We do know from the data that people can see expired offers. Therefore, I just have to make a judgement call, and it is easier atm to just stick with allocating the view to the first receive, as my code already does that atm. There is no right or wrong.\n",
    "### -------------------------------------   End of investigation  ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de649e",
   "metadata": {},
   "source": [
    "### Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the sub-block labels into master_interim, which is a temporary copy of master\n",
    "master_interim = pd.merge(master, df_flat_list, how = 'left', left_index = True, right_on = 'index')\n",
    "# Re-add indices from master to master_interim\n",
    "master_interim.index = master.index\n",
    "# reassign \"master\"\n",
    "master = master_interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923f218",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean out columns with obsolete views\n",
    "master = master.loc[master['sub_block'] != 9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b0036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new block index from the original person_offer_id and the sub_blocks that uniquely identifies all sub-blocks\n",
    "block_id_lookup = master[['person_offer_id','sub_block']].drop_duplicates().reset_index(drop=True)\n",
    "block_id_lookup = block_id_lookup.reset_index().rename(columns={'index':'block_id'})\n",
    "\n",
    "# Set a dummy value for NaN rows (the transaction rows) >> Not needed as no transactions in df anymore\n",
    "# block_id_lookup.loc[(block_id_lookup['person_offer_id'].isna()) & (block_id_lookup['sub_block'].isna()), 'block_id'] = 99999\n",
    "\n",
    "# Merge the new index into the dataframe\n",
    "master = pd.merge(master, block_id_lookup, how = 'left', left_on = ['person_offer_id','sub_block'], right_on = ['person_offer_id','sub_block'])\n",
    "\n",
    "# Remove obsolete indices\n",
    "master = master.drop(['person_offer_id', 'sub_block','index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e0cac",
   "metadata": {},
   "source": [
    "##### Data checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07490168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Datacheck 1: No more than 3 rows per block_id (except transactions)\n",
    "check1 = master.groupby(by=['block_id']).size().sort_values()\n",
    "\n",
    "check1 = pd.DataFrame(check1)\n",
    "\n",
    "check1.groupby(by=[0]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394a065",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Datacheck 2: Only 0 or 1 of each event type per block_id (except transactions)\n",
    "check2 = master[['block_id','event']]\n",
    "\n",
    "check2 = check2.pivot_table(index='block_id', columns = 'event',aggfunc=len, fill_value=0)\n",
    "\n",
    "check2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a91390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datacheck 3: Every block has a receive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a12c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "check2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4e261",
   "metadata": {},
   "source": [
    "##### in-between step so I don't have to rerun code every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5548a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "master.to_csv(r'2021_12_08_interim_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a8656bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "master = pd.read_csv(r'2021_12_08_interim_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdeb146",
   "metadata": {},
   "source": [
    "In the next step, I need to transform the dataframe so that each sub-block only has one row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423ec07",
   "metadata": {},
   "source": [
    "#### Step 3.2 transform the table so that each block has one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6abc8f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating pivot with time of event\n",
    "master_pivot = master.pivot_table(index='block_id', columns = 'event', values = 'time', fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc1b52f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Keep only one row with all relevant values per offer (the max statement does that)\n",
    "master_collapse = master[['block_id','person','event_offer_id','event_reward','offer_type','offer_reward','offer_duration','offer_valid_until','offer_difficulty','email','web','social','mobile']]\n",
    "master_collapse = master_collapse.groupby(by=['block_id']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c438e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the collapsed master frame with the event - time information from the master_pivot\n",
    "master = pd.merge(master_pivot, master_collapse, how = 'left', right_index = True, left_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ff1e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer completed</th>\n",
       "      <th>offer received</th>\n",
       "      <th>offer viewed</th>\n",
       "      <th>person</th>\n",
       "      <th>event_offer_id</th>\n",
       "      <th>event_reward</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>offer_reward</th>\n",
       "      <th>offer_duration</th>\n",
       "      <th>offer_valid_until</th>\n",
       "      <th>offer_difficulty</th>\n",
       "      <th>email</th>\n",
       "      <th>web</th>\n",
       "      <th>social</th>\n",
       "      <th>mobile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>block_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576.0</td>\n",
       "      <td>576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>2906b810c7d4411798c6938adc9daaa5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>744.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>336</td>\n",
       "      <td>372.0</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>168</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>414.0</td>\n",
       "      <td>408</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>f19421c1d4aa40978ebb69ca19b0e20d</td>\n",
       "      <td>5.0</td>\n",
       "      <td>bogo</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>528.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528.0</td>\n",
       "      <td>504</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>2</td>\n",
       "      <td>240</td>\n",
       "      <td>744.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>168</td>\n",
       "      <td>216.0</td>\n",
       "      <td>00116118485d4dfda04fdbaba9a87b5c</td>\n",
       "      <td>f19421c1d4aa40978ebb69ca19b0e20d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bogo</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>288.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>576</td>\n",
       "      <td>630.0</td>\n",
       "      <td>00116118485d4dfda04fdbaba9a87b5c</td>\n",
       "      <td>f19421c1d4aa40978ebb69ca19b0e20d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bogo</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>696.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>576.0</td>\n",
       "      <td>408</td>\n",
       "      <td>432.0</td>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>5</td>\n",
       "      <td>240</td>\n",
       "      <td>648.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>252.0</td>\n",
       "      <td>168</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>discount</td>\n",
       "      <td>3</td>\n",
       "      <td>168</td>\n",
       "      <td>336.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          offer completed  offer received  offer viewed  \\\n",
       "block_id                                                  \n",
       "0                   576.0             576           NaN   \n",
       "1                     NaN             336         372.0   \n",
       "2                     NaN             168         192.0   \n",
       "3                   414.0             408         456.0   \n",
       "4                   528.0             504         540.0   \n",
       "5                     NaN             168         216.0   \n",
       "6                     NaN             576         630.0   \n",
       "7                   576.0             408         432.0   \n",
       "8                   252.0             168         186.0   \n",
       "9                     NaN               0           6.0   \n",
       "\n",
       "                                    person                    event_offer_id  \\\n",
       "block_id                                                                       \n",
       "0         0009655768c64bdeb2e877511632db8f  2906b810c7d4411798c6938adc9daaa5   \n",
       "1         0009655768c64bdeb2e877511632db8f  3f207df678b143eea3cee63160fa8bed   \n",
       "2         0009655768c64bdeb2e877511632db8f  5a8bc65990b245e5a138643cd4eb9837   \n",
       "3         0009655768c64bdeb2e877511632db8f  f19421c1d4aa40978ebb69ca19b0e20d   \n",
       "4         0009655768c64bdeb2e877511632db8f  fafdcd668e3743c1bb461111dcafc2a4   \n",
       "5         00116118485d4dfda04fdbaba9a87b5c  f19421c1d4aa40978ebb69ca19b0e20d   \n",
       "6         00116118485d4dfda04fdbaba9a87b5c  f19421c1d4aa40978ebb69ca19b0e20d   \n",
       "7         0011e0d4e6b944f998e987f904e8c1e5  0b1e1539f2cc45b7b9fa7c272da2e1d7   \n",
       "8         0011e0d4e6b944f998e987f904e8c1e5  2298d6c36e964ae4a3e7e9706d1fb8c2   \n",
       "9         0011e0d4e6b944f998e987f904e8c1e5  3f207df678b143eea3cee63160fa8bed   \n",
       "\n",
       "          event_reward     offer_type  offer_reward  offer_duration  \\\n",
       "block_id                                                              \n",
       "0                  2.0       discount             2             168   \n",
       "1                  NaN  informational             0              96   \n",
       "2                  NaN  informational             0              72   \n",
       "3                  5.0           bogo             5             120   \n",
       "4                  2.0       discount             2             240   \n",
       "5                  NaN           bogo             5             120   \n",
       "6                  NaN           bogo             5             120   \n",
       "7                  5.0       discount             5             240   \n",
       "8                  3.0       discount             3             168   \n",
       "9                  NaN  informational             0              96   \n",
       "\n",
       "          offer_valid_until  offer_difficulty  email  web  social  mobile  \n",
       "block_id                                                                   \n",
       "0                     744.0                10      1    1       0       1  \n",
       "1                     432.0                 0      1    1       0       1  \n",
       "2                     240.0                 0      1    0       1       1  \n",
       "3                     528.0                 5      1    1       1       1  \n",
       "4                     744.0                10      1    1       1       1  \n",
       "5                     288.0                 5      1    1       1       1  \n",
       "6                     696.0                 5      1    1       1       1  \n",
       "7                     648.0                20      1    1       0       0  \n",
       "8                     336.0                 7      1    1       1       1  \n",
       "9                      96.0                 0      1    1       0       1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a73d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master[['event_offer_id','person','offer received','offer viewed','offer completed','event_reward','offer_type','offer_reward','offer_duration','offer_valid_until','offer_difficulty','email','web','social','mobile']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02071fb0",
   "metadata": {},
   "source": [
    "### Step 4: Create derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcee6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# was valid until (either expiry date or completion date)\n",
    "master['factual_validity_end'] = master[['offer completed','offer_valid_until']].min(axis = 1)\n",
    "\n",
    "# was completed\n",
    "master['was_completed'] = 1\n",
    "master.loc[master['offer completed'].isna(),'was_completed'] = 0\n",
    "\n",
    "# was aware binary\n",
    "master['was_aware'] = 0\n",
    "master.loc[(master['offer viewed'] != np.nan) & (master['offer viewed'] < master['factual_validity_end']), 'was_aware'] = 1\n",
    "\n",
    "# time taken to become aware\n",
    "master['time_lag_awareness'] = np.nan\n",
    "master.loc[master['was_aware'] == 1, 'time_lag_awareness'] = master['offer viewed'] - master['offer received']\n",
    "\n",
    "# time taken to complete when aware\n",
    "master['time_lag_completion'] = np.nan\n",
    "master.loc[(master['was_aware'] == 1) & (master['was_completed'] == 1), 'time_lag_completion'] = master['offer completed'] - master['offer viewed']\n",
    "\n",
    "# time left between completion and natural expiry when aware\n",
    "master['time_lag_completion_to_expiry'] = np.nan\n",
    "master.loc[(master['was_aware'] == 1) & (master['was_completed'] == 1), 'time_lag_completion_to_expiry'] = master['offer_valid_until'] - master['offer completed'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad159f0",
   "metadata": {},
   "source": [
    "The awareness period for offers with offer awareness is 'offer viewed' to 'factual_validity_end'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e62173",
   "metadata": {},
   "source": [
    "### Step 5: Insert transaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ec8d8",
   "metadata": {},
   "source": [
    "#### 5.1: Set up transaction df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fb8ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up transaction dataframe\n",
    "tx = transcript.loc[transcript['event'] == 'transaction'].reset_index().rename(columns={'index':'tx_id','time':'time_of_transaction'})\n",
    "tx = pd.concat([tx, pd.DataFrame(tx['value'].tolist())], axis=1) # Extract the amount value\n",
    "tx = tx.drop(['event','value'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1d70939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tx_id</th>\n",
       "      <th>person</th>\n",
       "      <th>time_of_transaction</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12654</td>\n",
       "      <td>02c083884c7d45b39cc68e1314fec56c</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12657</td>\n",
       "      <td>9fa9ae8f57894cc9a3b8a9bbe0fc1b2f</td>\n",
       "      <td>0</td>\n",
       "      <td>34.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12659</td>\n",
       "      <td>54890f68699049c2a04d415abc25e717</td>\n",
       "      <td>0</td>\n",
       "      <td>13.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12670</td>\n",
       "      <td>b2f1cd155b864803ad8334cdf13c4bd2</td>\n",
       "      <td>0</td>\n",
       "      <td>19.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12671</td>\n",
       "      <td>fe97aa22dd3e48c8b143116a8403dd52</td>\n",
       "      <td>0</td>\n",
       "      <td>18.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tx_id                            person  time_of_transaction  amount\n",
       "0  12654  02c083884c7d45b39cc68e1314fec56c                    0    0.83\n",
       "1  12657  9fa9ae8f57894cc9a3b8a9bbe0fc1b2f                    0   34.56\n",
       "2  12659  54890f68699049c2a04d415abc25e717                    0   13.23\n",
       "3  12670  b2f1cd155b864803ad8334cdf13c4bd2                    0   19.51\n",
       "4  12671  fe97aa22dd3e48c8b143116a8403dd52                    0   18.97"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d576b6f",
   "metadata": {},
   "source": [
    "#### 5.2 Set up awareness dataframe\n",
    "\n",
    "Each transaction needs to be allocated to one or more block ids. To achieve this, the awareness dataframe is created that contains one row for each point in time t that a person was aware of an offer. Then this is merged onto the transaction dataframe from step 5.1 from this df one can then see if a transaction happened during aware or anaware time and if it was during unaware time if the person was only aware of one offer or of multiple offers.\n",
    "If a person was aware of multiple offers, the transaction gets allocated based on the following logic:\n",
    "* The transaction gets allocated to the first offer that was completed because then the spend clearly helped to complete that offer\n",
    "* If no offer was completed then the amount is split evenly\n",
    "* More complex scenarios are possible e.g. that with one transaction a person completed two offers at the same time However, such special cases are not considered in the data wrangling as their number is estimated to be few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22b65b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the awareness frame - only keep offers that a person was aware of. All other time is unaware time.\n",
    "aw = master[['person','event_offer_id','offer viewed','factual_validity_end','was_aware']].loc[master['was_aware']==1].reset_index().drop('was_aware', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a1fa097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aw['rows_needed'] = aw['factual_validity_end'] - aw['offer viewed'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b91595da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with block ids and number of rows needed per block id\n",
    "block_ids, lines = aw['block_id'],aw['rows_needed']\n",
    "dict1 = pd.Series(lines.values, block_ids.values).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2cd7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all block ids to use in list comprehension\n",
    "block_id_list = aw['block_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d03b207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with number of each block id appearing as many times as rows are needed\n",
    "# Example: block id 1 had awareness for 10 hours >> For block id 1, 10 rows are needed >> block id 1 appears 10 times in the list\n",
    "row_list = [[i for j in range(int(dict1[i]))] for i in block_id_list]\n",
    "\n",
    "# Flatten the list as is currently list of lists\n",
    "# Source Stackoverflow: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
    "# Question by Emma: https://stackoverflow.com/users/110527/emma\n",
    "# Answer by Alex Martelli: https://stackoverflow.com/users/95810/alex-martelli\n",
    "row_list = [i for sublist in row_list for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ea2ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the original information \n",
    "aw_expanded = pd.merge(pd.DataFrame(row_list).rename(columns={0:'block_id'}), aw, how = 'left', left_on = 'block_id', right_on = 'block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3bd73b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use SQL to add row numbers that can then be transformed to hours\n",
    "aw_expanded = sqldf(\"SELECT *, row_number() over (partition by block_id) as counter FROM aw_expanded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a3178a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate columns with separate hours\n",
    "aw_hourly = aw_expanded\n",
    "aw_hourly['hour'] = aw_hourly['counter'] + aw_hourly['offer viewed'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "968ca8a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aw_hourly = aw_hourly[['block_id','person','event_offer_id','hour','offer viewed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80e7fe",
   "metadata": {},
   "source": [
    "#### 5.3 Combine both dataframes, create control and treatment frame and allocate block ids to transactions where a customer was aware ( = treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a86950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_block_ids = pd.merge(tx, aw_hourly, how = 'left', left_on = ['person','time_of_transaction'], right_on =['person','hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b6c1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the maximum how many blocks a transaction could be allocated to\n",
    "count_tx_ids = tx_block_ids[['tx_id']].groupby(by=['tx_id']).size()\n",
    "count_tx_ids.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbeb967d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    133710\n",
       "2      5117\n",
       "3       126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many overlap cases exist?\n",
    "pd.DataFrame(count_tx_ids).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5fb5e",
   "metadata": {},
   "source": [
    "##### Separate the transactions that happened during non-awareness (control) from those that happened during awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6cfbcfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split tx_block_ids into a control and treatment frameb\n",
    "tx_control = tx_block_ids.loc[tx_block_ids['event_offer_id'].isna()] # Not aware or no active offer\n",
    "tx_treatment = tx_block_ids.loc[~tx_block_ids['event_offer_id'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053beb9",
   "metadata": {},
   "source": [
    "##### Separate the non-overlap treatment cases from the overlap treatment cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a7d8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify transaction ids with multiple block_ids, i.e. those with more than 1 row in the tx_treatment dataframe\n",
    "tx_treatment_gr = tx_treatment[['tx_id']].groupby(by=['tx_id']).size()\n",
    "tx_treatment_block_id_count = pd.merge(tx_treatment, pd.DataFrame(tx_treatment_gr).rename(columns={0:'count'}), how = 'left', left_on = 'tx_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "917cd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cases without overlap and store in separate df as these need further treatment\n",
    "tx_treatment_no_overlap = tx_treatment_block_id_count.loc[tx_treatment_block_id_count['count'] == 1]\n",
    "tx_treatment_overlap = tx_treatment_block_id_count.loc[tx_treatment_block_id_count['count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b55f8037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the non-overlap treatment cases where transaction id is the key and block id is the value\n",
    "id_tx_fordict, block_id_fordict = tx_treatment_no_overlap['tx_id'],tx_treatment_no_overlap['block_id']\n",
    "dict_treatment_no_overlap = pd.Series(block_id_fordict.values, id_tx_fordict.values).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bd02c",
   "metadata": {},
   "source": [
    "##### Allocate transaction ids with offer overlap to the correct offer(s) (i.e. block_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d742fee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add information on whether an offer was completed\n",
    "tx_treatment_overlap = pd.merge(tx_treatment_overlap, master[['was_completed','offer completed']], how = 'left', left_on = 'block_id', right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a98b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: dfriends.com by Dan Friedman, April 15, 2019: https://dfrieds.com/data-analysis/rank-method-python-pandas.html\n",
    "# Add ranks to all completed block_ids and give those that are completed simultaneously the same rank\n",
    "tx_treatment_overlap['rank'] = tx_treatment_overlap.groupby(['tx_id'])['offer completed'].rank(method = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "921a3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe within tx id by time the offer was viewed\n",
    "tx_treatment_overlap = tx_treatment_overlap.sort_values(by=['tx_id','rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c97ab686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all transaction ids that need to be processed due to offer overlap\n",
    "tx_ids_with_overlap = tx_treatment_overlap['tx_id'].tolist()\n",
    "# Remove all duplicates\n",
    "tx_ids_with_overlap = list(set(tx_ids_with_overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5dfcdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate block ids to transaction ids; in the dict, the key will be the transaction id and the value the block id\n",
    "# If a transaction needs to be split between multiple offers, then there will be a list of offer ids\n",
    "\n",
    "dict_treatment_overlap = {}\n",
    "tx_treatment_overlap_reduced = tx_treatment_overlap[['tx_id','block_id','was_completed','rank']]\n",
    "for tx_id in tx_ids_with_overlap:\n",
    "    # Reduce dataframe to dataframe of interest\n",
    "    df = tx_treatment_overlap_reduced.loc[tx_treatment_overlap_reduced['tx_id'] == tx_id]\n",
    "    sum_completed = df['was_completed'].sum()\n",
    "    if sum_completed == 0:\n",
    "        dict_treatment_overlap[tx_id] = df['block_id'].tolist()\n",
    "    elif sum_completed > 0:\n",
    "        dict_treatment_overlap[tx_id] = df['block_id'].loc[df['rank'] == 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65e3aa",
   "metadata": {},
   "source": [
    "#### 5.4 Create a final awareness (=treatment) dataframe with one row per transaction id and block id as well as the correct amount to be allocated to the block id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "837a996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that allocates transactions to block ids for those with and without awareness and those with and without overlap\n",
    "# https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe\n",
    "block_id_allocation = pd.DataFrame(dict_treatment_overlap.items()).set_index(0).rename(columns={1:0}).append(pd.DataFrame.from_dict(dict_treatment_no_overlap, orient = 'index'))\n",
    "block_id_allocation = block_id_allocation.reset_index().rename(columns = {'index':'tx_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d13fdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Data check for duplicates; needs to be 1\n",
    "print(block_id_allocation.groupby(by='tx_id').size().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3189776b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge into tx dataframe\n",
    "block_id_allocation = pd.merge(tx, block_id_allocation, how = 'left', left_on = 'tx_id', right_on = 'tx_id').rename(columns = {0:'block_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6ee4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of block_ids that a tx_id is allocated to to reduce the amount\n",
    "dict_treatment_overlap_len = {}\n",
    "for key, value in dict_treatment_overlap.items():\n",
    "    try:\n",
    "        a = len(value)\n",
    "    except:\n",
    "        a = 1\n",
    "    dict_treatment_overlap_len[key] = a\n",
    "\n",
    "block_id_allocation = pd.merge(block_id_allocation,pd.DataFrame.from_dict(dict_treatment_overlap_len, orient = 'index'), how = 'left', left_on = 'tx_id', right_index = True).rename(columns={0:'divider'})\n",
    "block_id_allocation.loc[block_id_allocation['divider'].isna(), 'divider'] = 1\n",
    "\n",
    "# Calculate amount to be allocated to each separate block_id\n",
    "block_id_allocation['corrected_amount'] = block_id_allocation['amount']/block_id_allocation['divider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "25c13469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Expand the df to have multiple rows where there are lists with multiple block_ids for a tx_id\n",
    "# https://stackoverflow.com/questions/38203352/expand-pandas-dataframe-column-into-multiple-rows\n",
    "block_id_allocation = block_id_allocation.explode('block_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d048b",
   "metadata": {},
   "source": [
    "#### 5.5 Aggregate the transactions by block_id and join into the master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb0ed413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amount_per_block_id = block_id_allocation. groupby(['block_id'])['corrected_amount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "aw_final = pd.merge(master, amount_per_block_id, how = 'left', left_on = 'block_id', right_on = 'block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ecf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Checks:\n",
    "# Check 1: Amounts are only allocated to aware offers\n",
    "aw_final['was_aware'].loc[~aw_final['corrected_amount'].isna()].min() # >> Should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e139ab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_offer_id</th>\n",
       "      <th>person</th>\n",
       "      <th>offer received</th>\n",
       "      <th>offer viewed</th>\n",
       "      <th>offer completed</th>\n",
       "      <th>event_reward</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>offer_reward</th>\n",
       "      <th>offer_duration</th>\n",
       "      <th>offer_valid_until</th>\n",
       "      <th>...</th>\n",
       "      <th>web</th>\n",
       "      <th>social</th>\n",
       "      <th>mobile</th>\n",
       "      <th>factual_validity_end</th>\n",
       "      <th>was_completed</th>\n",
       "      <th>was_aware</th>\n",
       "      <th>time_lag_awareness</th>\n",
       "      <th>time_lag_completion</th>\n",
       "      <th>time_lag_completion_to_expiry</th>\n",
       "      <th>corrected_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>block_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [event_offer_id, person, offer received, offer viewed, offer completed, event_reward, offer_type, offer_reward, offer_duration, offer_valid_until, offer_difficulty, email, web, social, mobile, factual_validity_end, was_completed, was_aware, time_lag_awareness, time_lag_completion, time_lag_completion_to_expiry, corrected_amount]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check 2: All completed offers where someone was aware have an amount\n",
    "aw_final.loc[(aw_final['was_completed'] == 1) & (aw_final['corrected_amount'].isna()) & (aw_final['was_aware'] == 1)] # >> Should return no results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2408ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unaware block_ids from the final awareness dataframe again after wrangling has passed checks\n",
    "aw_final = aw_final.loc[aw_final['was_aware'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6814a",
   "metadata": {},
   "source": [
    "### Step 6: Create hourly average spend\n",
    "Daily averages for the treatment group are on person - offer level.\n",
    "Daily averages for the control group are per person, counting all unaware time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ff177",
   "metadata": {},
   "source": [
    "#### Step 6.1: Treatment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "56382dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add duration of awareness by hour to aw_final\n",
    "aw_final = pd.merge(aw_final,aw[['block_id','rows_needed']], how = 'left', left_index = True, right_on = 'block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3b4e63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_hourly_averages = aw_final.groupby(by=['person', 'event_offer_id']).agg({'corrected_amount':sum, 'rows_needed':sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d4cb7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: I currently have too many rows because validity can exceed experiment timeframe\n",
    "treatment_hourly_averages['hourly_avg'] = treatment_hourly_averages['corrected_amount']/treatment_hourly_averages['rows_needed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a70f0",
   "metadata": {},
   "source": [
    "#### Step 6.2: Control Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3ff99d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total hours of experiment\n",
    "max_hour = transcript['time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f583f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41415017/count-unique-values-using-pandas-groupby/41415028\n",
    "person_aware_time = aw_hourly.loc[aw_hourly['hour'] <= max_hour].groupby('person')['hour'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "02d6c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_unaware_time = (pd.DataFrame(person_aware_time)['hour'] - max_hour) * (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "57e882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time every person was unaware/had no active offer\n",
    "control_hourly_averages = pd.merge(pd.DataFrame(person_unaware_time), pd.DataFrame(tx_control.groupby('person')['amount'].sum()), left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6679e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_hourly_averages['hourly_avg'] = control_hourly_averages['amount']/control_hourly_averages['hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a2567874",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour          0\n",
       "amount        0\n",
       "hourly_avg    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data check\n",
    "control_hourly_averages.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94429ad",
   "metadata": {},
   "source": [
    "  \n",
    "Ideas for further variables I could construct and use for prediction are:\n",
    "\n",
    "    * Was previously aware of offer\n",
    "    * Previously received purely informational offer\n",
    "    * Time since last offer awareness\n",
    "    * Time since last offer completion\n",
    "\n",
    "\n",
    "\n",
    "# NEXT STEPS:\n",
    "Important: Need to cut the awareness hour timeframes to max 714! (aw_hourly).\n",
    "I think that is somewhere very much in the beginning, where I calculate the maximum offer validity. There I should set a max.\n",
    "\n",
    "Create avg daily spend per\n",
    "- person in control group\n",
    "- block_id in awareness (treatment) df\n",
    "\n",
    "### Next stage:\n",
    "Check distribution of key variables between those with and without demographic information, and drop them if no massive differences.\n",
    "\n",
    "### Next stage: \n",
    "Show some descriptive statistics to understand your data. specifically average spend charts and offer completion rates. For average spend, also look at those who complete versus do not complete versus control, because I want to know if simple awareness already changes spend.Consider how to handle the purely informational offers.\n",
    "\n",
    "### Next big stage:\n",
    "Run t-tests or ANOVAS or even a GLM to assess success of each offer. measure success as avg spend per hour compared to non-aware times. Your separate datapoints are person-x-offerX and person-x-unaware_time, where each shows average hourly spend.\n",
    "\n",
    "Then look at offer completion rates when aware, i.e. conversion. Do ANOVA with comparsion between offers (or whatever the equivalent is, because this will be binary data) if you feel like it.\n",
    "\n",
    "### Next big stage:\n",
    "Select best offer. Reduce sample to people who have received that offer. Predict either offer conversion (i.e. uptake/completion) or predict increase in spend. If spend only really increases for people who complete an offer, run prediction on completion only. If spend also increases when people are just aware, then predict spend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1974b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Merging profile table\n",
    "master = pd.merge(master, profile[['gender','age','became_member_on','membership_duration','income']], how = 'left', left_on = master['person'], right_on = profile['id'])\n",
    "master = master.drop('key_0', axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
